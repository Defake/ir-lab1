{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "def get_file_content(fpath, enc='utf-8'):\n",
    "    file = open(fpath, \"r\") #, encoding=enc\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    return content.split('\\n')\n",
    "\n",
    "def spit(fpath, content):\n",
    "    path = fpath\n",
    "    file = open(fpath, \"w\", encoding='utf-8')\n",
    "    file.write(content)\n",
    "    file.close()\n",
    "\n",
    "filenames = []\n",
    "news_regex = r\"^([^\\s\\r\\n]+)\\n\\d+\\n[^\\s\\r\\n]+\\n\\n(#\\w+\\s)+\\n=+\\n.*$\"\n",
    "for filename in glob.iglob('/Users/defake/Documents/LentaDS/lenta_news_wo_author/**/*.txt', recursive=True):\n",
    "    if os.path.isfile(filename):\n",
    "        filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_content(content):\n",
    "    if content == '' or content == ['']:\n",
    "            return None\n",
    "    name = content[0].strip()\n",
    "    #number = content[1]\n",
    "    #time = content[2]\n",
    "    tags = re.findall(r\"#(\\w+)\", content[4].lower())\n",
    "    text = content[6].lower().strip()\n",
    "    return {'name': name,\n",
    "            'tags': tags,\n",
    "            'text': text}\n",
    "\n",
    "parsed = [parse_file_content(get_file_content(f)) for f in filenames]\n",
    "parsed = [x for x in parsed if x != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate number of topics\n",
    "topics = set()\n",
    "for news in parsed:\n",
    "    for t in news['tags']:\n",
    "        if t not in topics:\n",
    "            topics.add(t)\n",
    "\n",
    "topics_n = len(topics)\n",
    "topics_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сегодня',\n",
       " 'областной',\n",
       " 'центр',\n",
       " 'сахалин',\n",
       " 'курить',\n",
       " 'получить',\n",
       " 'статус',\n",
       " 'очаг',\n",
       " 'распространение',\n",
       " 'холера']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pymorphy2\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('russian'))\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "tokenized_documents = [[morph.parse(word)[0].normal_form for word in re.findall(r'\\w+', news['text']) if word not in stopwords] for news in parsed[:-1000]]\n",
    "del filenames\n",
    "del topics\n",
    "del parsed\n",
    "tokenized_documents[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 3),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 2),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 2),\n",
       " (57, 2),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 3),\n",
       " (62, 3),\n",
       " (63, 2),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 2),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 2),\n",
       " (84, 1),\n",
       " (85, 2),\n",
       " (86, 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(tokenized_documents)\n",
    "corpus = [dictionary.doc2bow(document) for document in tokenized_documents]\n",
    "\n",
    "del tokenized_documents\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Write the corpus and the dictionary to a file for a case...\n",
    "\n",
    "with open('./bow_corpus', 'wb') as f:\n",
    "    pickle.dump(corpus, f)\n",
    "    \n",
    "dictionary.save('./news_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "with open('./bow_corpus', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "dictionary = Dictionary.load('./news_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 3),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 3),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 2),\n",
       " (65, 2),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 4),\n",
       " (70, 3),\n",
       " (71, 2),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 2),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 3),\n",
       " (92, 1),\n",
       " (93, 2),\n",
       " (94, 4)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lda ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_keywords(topic_terms):\n",
    "    print(\"Keywords for topic:\")\n",
    "    for kwd, p in topic_terms:\n",
    "        print(dictionary.id2token[int(kwd)])\n",
    "\n",
    "def get_topic_for_text(model, text):\n",
    "    doc = [morph.parse(word)[0].normal_form for word in re.findall(r'\\w+', text.lower()) if word not in stopwords]\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    topics = model[bow]\n",
    "    topic_num = max(topics, key=lambda x: x[1])\n",
    "    return topic_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "lda = LdaModel(corpus, num_topics=topics_n, chunksize=2000, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['во вторник верховный суд якутии возобновил процесс по делу об отмене регистрации кандидатом в президенты действующего главы республики михаила николаева, передает агентство \"интерфакс\" со ссылкой на источник в верховном суде. как сообщалось ранее, процесс был инициирован двумя жителями республики, однако приостанавливался в связи с запросом представителей николаева в конституционный суд рф.']\n",
      "Keywords for topic:\n",
      "россия\n",
      "президент\n",
      "год\n",
      "новость\n",
      "который\n",
      "риа\n",
      "владимир\n",
      "правительство\n",
      "глава\n",
      "депутат\n"
     ]
    }
   ],
   "source": [
    "test_doc = get_file_content('./test_news1.txt')\n",
    "print(test_doc)\n",
    "test_doc = ''.join(get_file_content('./test_news1.txt'))\n",
    "prob_topic = get_topic_for_text(lda, test_doc)\n",
    "print_keywords(lda.get_topic_terms(prob_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsi = LsiModel(corpus, num_topics=topics_n, chunksize=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords for topic:\n",
      "который\n",
      "президент\n",
      "заявить\n",
      "сообщать\n",
      "израильский\n",
      "сша\n",
      "израиль\n",
      "палестинский\n",
      "министр\n",
      "страна\n"
     ]
    }
   ],
   "source": [
    "prob_topic = get_topic_for_text(lsi, test_doc)\n",
    "print_keywords(lda.get_topic_terms(prob_topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
